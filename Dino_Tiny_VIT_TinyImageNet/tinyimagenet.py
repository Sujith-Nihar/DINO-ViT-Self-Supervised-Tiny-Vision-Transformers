# -*- coding: utf-8 -*-
"""TinyImageNet.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1J2zCHM4mr6U-ss7cZ50q64E0m4zTUI0v
"""

!pip -q install timm

import os, shutil
from collections import defaultdict

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader

import torchvision
from torchvision import datasets, transforms

from tqdm.auto import tqdm
import timm

device = "cuda" if torch.cuda.is_available() else "cpu"
print("Device:", device)

CFG = {
    "image_size": 128,
    "batch_size": 256,
    "num_workers": 0,
    "epochs": 300,
    "lr": 1e-4,
    "weight_decay": 1e-6,
    "teacher_momentum": 0.996,
    "n_global_crops": 2,
    "n_local_crops": 2,
}

# Download TinyImageNet
!wget -q http://cs231n.stanford.edu/tiny-imagenet-200.zip
!unzip -q tiny-imagenet-200.zip

TINY_ROOT = "/content/tiny-imagenet-200"
print("TinyImageNet root:", TINY_ROOT)

def prepare_tiny_imagenet(root):
    """
    Make TinyImageNet val/ compatible with torchvision.datasets.ImageFolder.
    """
    val_dir = os.path.join(root, "val")
    images_dir = os.path.join(val_dir, "images")
    anno_file = os.path.join(val_dir, "val_annotations.txt")

    if not os.path.exists(images_dir):
        print("val/ already prepared, skipping.")
        return

    print("Preparing val/ ...")
    img_to_wnid = {}
    with open(anno_file, "r") as f:
        for line in f:
            parts = line.strip().split('\t')
            img_name, wnid = parts[0], parts[1]
            img_to_wnid[img_name] = wnid

    for img_name, wnid in img_to_wnid.items():
        class_dir = os.path.join(val_dir, wnid)
        os.makedirs(class_dir, exist_ok=True)

        src = os.path.join(images_dir, img_name)
        dst = os.path.join(class_dir, img_name)
        if os.path.exists(src) and not os.path.exists(dst):
            shutil.move(src, dst)

    if os.path.isdir(images_dir) and len(os.listdir(images_dir)) == 0:
        os.rmdir(images_dir)
    print("Done preparing val/.")

prepare_tiny_imagenet(TINY_ROOT)

IMAGENET_MEAN = (0.485, 0.456, 0.406)
IMAGENET_STD  = (0.229, 0.224, 0.225)

class DinoMultiCropTransform:
    """
    DINO-style multi-crop augmentation:
    - 2 global crops
    - n_local_crops local crops
    """
    def __init__(self, size=64, n_global=2, n_local=2, mean=IMAGENET_MEAN, std=IMAGENET_STD):
        self.n_global = n_global
        self.n_local = n_local
        self.size = size

        flip_color = transforms.Compose([
            transforms.RandomHorizontalFlip(p=0.5),
            transforms.RandomApply([
                transforms.ColorJitter(0.4, 0.4, 0.4, 0.1)
            ], p=0.8),
            transforms.RandomGrayscale(p=0.2),
        ])

        normalize = transforms.Compose([
            transforms.ToTensor(),
            transforms.Normalize(mean, std),
        ])

        # 2 global crops (scale 0.4–1.0)
        self.global_transforms = [
            transforms.Compose([
                transforms.RandomResizedCrop(
                    size, scale=(0.4, 1.0),
                    interpolation=transforms.InterpolationMode.BICUBIC,
                ),
                flip_color,
                transforms.GaussianBlur(kernel_size=15, sigma=(0.1, 2.0)),
                normalize,
            ]),
            transforms.Compose([
                transforms.RandomResizedCrop(
                    size, scale=(0.4, 1.0),
                    interpolation=transforms.InterpolationMode.BICUBIC,
                ),
                flip_color,
                transforms.GaussianBlur(kernel_size=15, sigma=(0.1, 2.0)),
                transforms.RandomSolarize(threshold=0.5, p=0.2),
                normalize,
            ]),
        ]

        # local crops (scale 0.08–0.4)
        self.local_transform = transforms.Compose([
            transforms.RandomResizedCrop(
                size, scale=(0.08, 0.4),
                interpolation=transforms.InterpolationMode.BICUBIC,
            ),
            flip_color,
            transforms.GaussianBlur(kernel_size=9, sigma=(0.1, 2.0)),
            normalize,
        ])

    def __call__(self, x):
        crops = []
        for i in range(self.n_global):
            crops.append(self.global_transforms[i](x))
        for _ in range(self.n_local):
            crops.append(self.local_transform(x))
        return crops

# Unlabeled TinyImageNet for self-supervised DINO
dino_transform = DinoMultiCropTransform(
    size=CFG["image_size"],
    n_global=CFG["n_global_crops"],
    n_local=CFG["n_local_crops"],
    mean=IMAGENET_MEAN,
    std=IMAGENET_STD,
)

tiny_train_unlabeled = datasets.ImageFolder(
    root=os.path.join(TINY_ROOT, "train"),
    transform=dino_transform,
)

pretrain_loader = DataLoader(
    tiny_train_unlabeled,
    batch_size=CFG["batch_size"],
    shuffle=True,
    num_workers=CFG["num_workers"],
    pin_memory=True,
    drop_last=True,
)

print("DINO pretrain images:", len(pretrain_loader.dataset))
print("Num classes (ignored for SSL):", len(tiny_train_unlabeled.classes))

# ---- ViT-Tiny backbone ----
#Create model
def create_vit_tiny(img_size):
    model = timm.create_model(
        "vit_tiny_patch16_224",
        img_size=img_size,
        num_classes=0,
    )
    return model

student_backbone = create_vit_tiny(CFG["image_size"]).to(device)
teacher_backbone = create_vit_tiny(CFG["image_size"]).to(device)

# start teacher as copy of student
teacher_backbone.load_state_dict(student_backbone.state_dict())
for p in teacher_backbone.parameters():
    p.requires_grad = False

embed_dim = student_backbone.num_features
print("ViT tiny embed dim:", embed_dim)


class DINOHead(nn.Module):
    def __init__(self, in_dim, out_dim=1024, hidden_dim=2048, bottleneck_dim=256, n_layers=3):
        super().__init__()
        layers = []
        dim_in = in_dim
        for i in range(n_layers - 1):
            layers.append(nn.Linear(dim_in, hidden_dim))
            layers.append(nn.GELU())
            dim_in = hidden_dim
        layers.append(nn.Linear(dim_in, bottleneck_dim))
        self.mlp = nn.Sequential(*layers)
        self.last_layer = nn.utils.weight_norm(nn.Linear(bottleneck_dim, out_dim, bias=False))
        self.last_layer.weight_g.data.fill_(1.0)

    def forward(self, x):
        x = self.mlp(x)
        x = F.normalize(x, dim=-1)
        x = self.last_layer(x)
        return x

out_dim = 1024  # dimension of DINO output/prototypes
student_head = DINOHead(embed_dim, out_dim=out_dim).to(device)
teacher_head = DINOHead(embed_dim, out_dim=out_dim).to(device)

teacher_head.load_state_dict(student_head.state_dict())
for p in teacher_head.parameters():
    p.requires_grad = False


class DINOLoss(nn.Module):
    def __init__(self, out_dim, n_global_crops, student_temp=0.1, teacher_temp=0.04, center_momentum=0.9):
        super().__init__()
        self.out_dim = out_dim
        self.n_global_crops = n_global_crops
        self.student_temp = student_temp
        self.teacher_temp = teacher_temp
        self.center_momentum = center_momentum
        self.register_buffer("center", torch.zeros(1, out_dim))

    def forward(self, student_outputs, teacher_outputs):
        """
        student_outputs: list of tensors [B, out_dim] for all crops (global+local)
        teacher_outputs: list of tensors [B, out_dim] for global crops only (length = n_global_crops)
        """
        # student log-probs
        student_logprobs = [
            F.log_softmax(s / self.student_temp, dim=-1)
            for s in student_outputs
        ]

        # teacher probs with centering
        teacher_probs = []
        for t in teacher_outputs:
            t = (t - self.center) / self.teacher_temp
            t = F.softmax(t, dim=-1)
            teacher_probs.append(t.detach())

        total_loss = 0.0
        n_loss_terms = 0

        # cross-entropy between teacher on global crops and student on all crops (except same view)
        for t_idx, t_prob in enumerate(teacher_probs):
            for s_idx, s_logprob in enumerate(student_logprobs):
                if s_idx == t_idx:
                    continue
                loss = -(t_prob * s_logprob).sum(dim=-1).mean()
                total_loss += loss
                n_loss_terms += 1

        total_loss /= n_loss_terms

        # update center
        batch_center = torch.cat(teacher_outputs, dim=0).mean(dim=0, keepdim=True)
        self.center = self.center * self.center_momentum + batch_center * (1 - self.center_momentum)

        return total_loss

dino_loss_fn = DINOLoss(out_dim=out_dim, n_global_crops=CFG["n_global_crops"]).to(device)

# Optimizer: only student params (backbone + head)
params = list(student_backbone.parameters()) + list(student_head.parameters())
optimizer = torch.optim.AdamW(params, lr=CFG["lr"], weight_decay=CFG["weight_decay"])

@torch.no_grad()
def update_teacher(student_backbone, teacher_backbone, student_head, teacher_head, m):
    for ps, pt in zip(student_backbone.parameters(), teacher_backbone.parameters()):
        pt.data.mul_(m).add_(ps.data, alpha=1.0 - m)
    for ps, pt in zip(student_head.parameters(), teacher_head.parameters()):
        pt.data.mul_(m).add_(ps.data, alpha=1.0 - m)

import time

num_epochs = CFG["epochs"]

for epoch in range(1, num_epochs + 1):
    student_backbone.train()
    student_head.train()
    teacher_backbone.eval()
    teacher_head.eval()

    running_loss = 0.0
    n_steps = 0

    # start timer for this epoch
    epoch_start = time.time()

    pbar = tqdm(pretrain_loader, desc=f"[DINO] Epoch {epoch}/{num_epochs}", leave=False)

    for multi_crops, _ in pbar:
        crops = [img.to(device, non_blocking=True) for img in multi_crops]
        n_global = CFG["n_global_crops"]

        # ---- teacher on global crops ----
        with torch.no_grad():
            teacher_out = []
            for x in crops[:n_global]:
                feat_t = teacher_backbone(x)
                if feat_t.dim() > 2:
                    feat_t = feat_t.mean(dim=(2, 3))
                teacher_out.append(teacher_head(feat_t))

        # ---- student on all crops ----
        student_out = []
        for x in crops:
            feat_s = student_backbone(x)
            if feat_s.dim() > 2:
                feat_s = feat_s.mean(dim=(2, 3))
            student_out.append(student_head(feat_s))

        loss = dino_loss_fn(student_out, teacher_out)

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        update_teacher(
            student_backbone, teacher_backbone,
            student_head, teacher_head,
            CFG["teacher_momentum"],
        )

        running_loss += loss.item()
        n_steps += 1
        pbar.set_postfix({"loss": f"{loss.item():.4f}"})

    epoch_loss = running_loss / n_steps
    epoch_secs = time.time() - epoch_start

    print(f"Epoch {epoch:3d}: loss={epoch_loss:.4f}, time={epoch_secs:.1f}s\
n")

from torchvision import transforms, datasets
from torch.utils.data import DataLoader

# transforms for evaluation / linear probe
eval_train_transform = transforms.Compose([
    transforms.RandomResizedCrop(CFG["image_size"], scale=(0.6, 1.0)),
    transforms.RandomHorizontalFlip(),
    transforms.ToTensor(),
    transforms.Normalize(IMAGENET_MEAN, IMAGENET_STD),
])

eval_val_transform = transforms.Compose([
    transforms.Resize(CFG["image_size"] + 8),
    transforms.CenterCrop(CFG["image_size"]),
    transforms.ToTensor(),
    transforms.Normalize(IMAGENET_MEAN, IMAGENET_STD),
])

tiny_train_eval = datasets.ImageFolder(
    root=os.path.join(TINY_ROOT, "train"),
    transform=eval_train_transform,
)

tiny_val_eval = datasets.ImageFolder(
    root=os.path.join(TINY_ROOT, "val"),
    transform=eval_val_transform,
)

num_classes = len(tiny_train_eval.classes)
print("TinyImageNet classes:", num_classes)
print("Train eval images:", len(tiny_train_eval))
print("Val eval images  :", len(tiny_val_eval))

train_eval_loader = DataLoader(
    tiny_train_eval,
    batch_size=CFG["batch_size"],
    shuffle=True,
    num_workers=CFG["num_workers"],
    pin_memory=True,
)

val_eval_loader = DataLoader(
    tiny_val_eval,
    batch_size=CFG["batch_size"],
    shuffle=False,
    num_workers=CFG["num_workers"],
    pin_memory=True,
)

import torch
from tqdm.auto import tqdm

@torch.no_grad()
def extract_features(backbone, loader, device):
    backbone.eval()
    all_feats = []
    all_labels = []

    for images, labels in tqdm(loader, desc="Extract", leave=False):
        images = images.to(device)
        labels = labels.to(device)

        feats = backbone(images)
        # if shape is (B, C, H, W) (just in case), pool & flatten
        if feats.dim() > 2:
            feats = feats.mean(dim=(2, 3))
        all_feats.append(feats.cpu())
        all_labels.append(labels.cpu())

    all_feats = torch.cat(all_feats, dim=0)
    all_labels = torch.cat(all_labels, dim=0)
    return all_feats, all_labels

!pip -q install scikit-learn

from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score

# use the DINO student backbone as frozen feature extractor
train_feats, train_labels = extract_features(student_backbone, train_eval_loader, device)
val_feats,   val_labels   = extract_features(student_backbone, val_eval_loader, device)

print("Train feats:", train_feats.shape)
print("Val feats  :", val_feats.shape)

k = 5
knn = KNeighborsClassifier(n_neighbors=k, metric="euclidean")
knn.fit(train_feats.numpy(), train_labels.numpy())

val_preds = knn.predict(val_feats.numpy())
knn_acc = accuracy_score(val_labels.numpy(), val_preds)
print(f"k-NN (k={k}) TinyImageNet val acc: {knn_acc*100:.2f}%")

import torch.nn as nn
import torch.optim as optim

class LinearProbe(nn.Module):
    def __init__(self, feat_dim, num_classes):
        super().__init__()
        self.fc = nn.Linear(feat_dim, num_classes)

    def forward(self, x):
        return self.fc(x)

# figure out feature dim from one batch
with torch.no_grad():
    student_backbone.eval()
    sample_imgs, _ = next(iter(train_eval_loader))
    sample_imgs = sample_imgs.to(device)
    sample_feats = student_backbone(sample_imgs)
    if sample_feats.dim() > 2:
        sample_feats = sample_feats.mean(dim=(2, 3))
    feat_dim = sample_feats.shape[1]

print("Feature dim:", feat_dim)

linear_probe = LinearProbe(feat_dim, num_classes).to(device)

# freeze backbone
for p in student_backbone.parameters():
    p.requires_grad = False

criterion_lp = nn.CrossEntropyLoss()
optimizer_lp = optim.Adam(linear_probe.parameters(), lr=1e-3)
epochs_lp = 300  # adjust

def train_linear_epoch(backbone, head, loader, optimizer, criterion, device):
    backbone.eval()
    head.train()

    running_loss = 0.0
    correct = 0
    total = 0

    for images, labels in tqdm(loader, desc="LP Train", leave=False):
        images = images.to(device)
        labels = labels.to(device)

        with torch.no_grad():
            feats = backbone(images)
            if feats.dim() > 2:
                feats = feats.mean(dim=(2, 3))

        logits = head(feats)
        loss = criterion(logits, labels)

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        running_loss += loss.item() * images.size(0)
        _, preds = logits.max(1)
        correct += preds.eq(labels).sum().item()
        total += labels.size(0)

    return running_loss / total, correct / total


@torch.no_grad()
def eval_linear_epoch(backbone, head, loader, criterion, device):
    backbone.eval()
    head.eval()

    running_loss = 0.0
    correct = 0
    total = 0

    for images, labels in tqdm(loader, desc="LP Val", leave=False):
        images = images.to(device)
        labels = labels.to(device)

        feats = backbone(images)
        if feats.dim() > 2:
            feats = feats.mean(dim=(2, 3))

        logits = head(feats)
        loss = criterion(logits, labels)

        running_loss += loss.item() * images.size(0)
        _, preds = logits.max(1)
        correct += preds.eq(labels).sum().item()
        total += labels.size(0)

    return running_loss / total, correct / total

import time

for epoch in range(1, epochs_lp + 1):
    print(f"[Linear Probe] Epoch {epoch}/{epochs_lp}:")

    epoch_start = time.time()

    train_loss, train_acc = train_linear_epoch(
        student_backbone, linear_probe,
        train_eval_loader, optimizer_lp, criterion_lp, device
    )
    val_loss, val_acc = eval_linear_epoch(
        student_backbone, linear_probe,
        val_eval_loader, criterion_lp, device
    )

    epoch_secs = time.time() - epoch_start

    print(
        f"Epoch {epoch:3d}: "
        f"train_loss={train_loss:.4f} | "
        f"train_acc={train_acc * 100:5.2f}% | "
        f"val_loss={val_loss:.4f}| "
        f"val_acc={val_acc * 100:5.2f}% | "
        f"time={epoch_secs:.1f}s\n"
    )

!pip install --quiet timm

import math
import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import matplotlib.pyplot as plt

from torchvision import datasets, transforms
from torch.utils.data import DataLoader
from PIL import Image

device = "cuda" if torch.cuda.is_available() else "cpu"
print("Device:", device)

# Data transforms (DINO-style ImageNet normalization)
img_transform = transforms.Compose([
    transforms.Resize(256, interpolation=Image.BICUBIC),
    transforms.CenterCrop(224),
    transforms.ToTensor(),
    transforms.Normalize(
        mean=(0.485, 0.456, 0.406),
        std=(0.229, 0.224, 0.225),
    ),
])

inv_normalize = transforms.Normalize(
    mean=[-0.485/0.229, -0.456/0.224, -0.406/0.225],
    std=[1/0.229, 1/0.224, 1/0.225],
)

stl_train = datasets.STL10(root="./data", split="train", download=True, transform=img_transform)
stl_test  = datasets.STL10(root="./data", split="test",  download=True, transform=img_transform)

train_loader = DataLoader(stl_train, batch_size=128, shuffle=True, num_workers=2)
test_loader  = DataLoader(stl_test,  batch_size=128, shuffle=False, num_workers=2)

print("STL-10 train:", len(stl_train), "test:", len(stl_test))

def tensor_to_pil(img_tensor):
    img = inv_normalize(img_tensor.cpu())
    img = torch.clamp(img, 0, 1)
    return transforms.ToPILImage()(img)

def show_side_by_side(images, titles=None, figsize=(12,4), cmap_list=None):
    n = len(images)
    plt.figure(figsize=figsize)
    for i, img in enumerate(images):
        plt.subplot(1, n, i+1)
        if isinstance(img, torch.Tensor):
            img = img.cpu().numpy()
            if img.ndim == 3:
                img = np.transpose(img, (1,2,0))
        if cmap_list is not None and cmap_list[i] is not None:
            plt.imshow(img, cmap=cmap_list[i])
        else:
            plt.imshow(img)
        if titles is not None:
            plt.title(titles[i])
        plt.axis("off")
    plt.tight_layout()
    plt.show()

def get_last_selfattention(model, img_batch):
    """
    img_batch: [B,3,H,W], normalized
    returns: [B, num_heads, N_tokens, N_tokens]
    """
    with torch.no_grad():
        attn = model.get_last_selfattention(img_batch)
    return attn

def visualize_dino_attention_random():
    # sample one image
    img_tensor, label = stl_test[np.random.randint(len(stl_test))]
    x = img_tensor.unsqueeze(0).to(device)         # [1,3,224,224]

    # get attention
    attn = get_last_selfattention(model, x)        # [1, heads, N, N]
    attn = attn[0]                                 # [heads,N,N]
    num_heads, N, _ = attn.shape

    num_patches = N - 1                            # ignore CLS
    grid_size = int(math.sqrt(num_patches))        # e.g., 14x14
    print("heads:", num_heads, "grid:", grid_size, "x", grid_size)

    # CLS → patch attention
    cls_attn = attn[:, 0, 1:]                      # [heads, num_patches]
    cls_attn = cls_attn / (cls_attn.max(dim=-1, keepdim=True).values + 1e-6)

    # reshape to spatial grid
    cls_maps = cls_attn.reshape(num_heads, grid_size, grid_size)

    # upsample to image size
    H, W = x.shape[2], x.shape[3]
    cls_maps_up = F.interpolate(
        cls_maps.unsqueeze(1),
        size=(H, W),
        mode="bilinear",
        align_corners=False
    ).squeeze(1)                                   # [heads,H,W]

    # average over heads
    avg_map = cls_maps_up.mean(dim=0)
    avg_map = (avg_map - avg_map.min()) / (avg_map.max() - avg_map.min() + 1e-6)

    # original image
    img_vis = tensor_to_pil(img_tensor)

    show_side_by_side(
        [img_vis, avg_map.cpu()],
        ["STL-10 image", "DINO attention (avg heads)"],
        figsize=(10,4),
        cmap_list=[None, "viridis"]
    )

    # optional: first few heads
    heads_to_show = min(4, num_heads)
    head_imgs = []
    head_titles = []
    cmaps = []
    for h in range(heads_to_show):
        hmap = cls_maps_up[h]
        hmap = (hmap - hmap.min()) / (hmap.max() - hmap.min() + 1e-6)
        head_imgs.append(hmap.cpu())
        head_titles.append(f"Head {h}")
        cmaps.append("viridis")

    show_side_by_side(
        head_imgs,
        head_titles,
        figsize=(14,3),
        cmap_list=cmaps
    )

visualize_dino_attention_random()

def attention_to_mask(attn_map, threshold=0.7):
    """
    attn_map: [H,W] torch tensor in [0,1]
    returns binary mask [H,W] in {0,1}
    """
    attn_np = attn_map.cpu().numpy()
    mask = (attn_np >= threshold).astype(np.float32)
    return mask

def visualize_attention_mask_random(threshold=0.7):
    img_tensor, label = stl_test[np.random.randint(len(stl_test))]
    x = img_tensor.unsqueeze(0).to(device)

    attn = get_last_selfattention(model, x)[0]
    num_heads, N, _ = attn.shape
    num_patches = N - 1
    grid = int(math.sqrt(num_patches))

    cls_attn = attn[:, 0, 1:]
    cls_attn = cls_attn / (cls_attn.max(dim=-1, keepdim=True).values + 1e-6)
    cls_maps = cls_attn.reshape(num_heads, grid, grid)

    H, W = x.shape[2], x.shape[3]
    cls_maps_up = F.interpolate(
        cls_maps.unsqueeze(1),
        size=(H, W),
        mode="bilinear",
        align_corners=False
    ).squeeze(1)

    avg_map = cls_maps_up.mean(dim=0)
    avg_map = (avg_map - avg_map.min()) / (avg_map.max() - avg_map.min() + 1e-6)

    mask_high = attention_to_mask(avg_map, threshold=threshold)
    mask_low  = 1.0 - mask_high

    img_denorm = inv_normalize(img_tensor).clamp(0,1).cpu().numpy()
    mask_high_3c = np.stack([mask_high]*3, axis=0)
    mask_low_3c  = np.stack([mask_low]*3, axis=0)

    img_high = img_denorm * mask_high_3c
    img_low  = img_denorm * mask_low_3c

    img_pil = tensor_to_pil(img_tensor)
    img_high_pil = transforms.ToPILImage()(torch.from_numpy(img_high))
    img_low_pil  = transforms.ToPILImage()(torch.from_numpy(img_low))

    show_side_by_side(
        [img_pil, avg_map.cpu(), img_high_pil, img_low_pil],
        ["Original", "Attention heat", "High-attn kept", "Low-attn kept"],
        figsize=(14,4),
        cmap_list=[None, "viridis", None, None]
    )

visualize_attention_mask_random(threshold=0.7)

@torch.no_grad()
def dino_features(model, x, layer_index=-1, use_cls=True):
    """
    Get features from an intermediate layer of DINO ViT.
    x: [B,3,H,W] normalized
    Returns: [B, C] (CLS or pooled patch tokens)
    """
    # returns list of layer outputs; we take the last by default
    feats = model.get_intermediate_layers(x, n=1)[0]   # [B, N, C]
    if use_cls:
        return feats[:, 0]                             # CLS token [B,C]
    else:
        return feats[:, 1:].mean(dim=1)                # mean over patches

from sklearn.neighbors import KNeighborsClassifier

# 1) Extract train features
train_feats = []
train_labels = []

for imgs, labels in DataLoader(stl_train, batch_size=128, shuffle=False):
    imgs = imgs.to(device)
    f = dino_features(model, imgs)   # [B, C]
    train_feats.append(f.cpu())
    train_labels.append(labels)

train_feats = torch.cat(train_feats, dim=0).numpy()
train_labels = torch.cat(train_labels, dim=0).numpy()
print("Train features:", train_feats.shape)

# 2) Fit k-NN
knn = KNeighborsClassifier(n_neighbors=20, metric="cosine")
knn.fit(train_feats, train_labels)

# 3) Extract test features + evaluate
test_feats = []
test_labels = []

for imgs, labels in test_loader:
    imgs = imgs.to(device)
    f = dino_features(model, imgs)
    test_feats.append(f.cpu())
    test_labels.append(labels)

test_feats = torch.cat(test_feats, dim=0).numpy()
test_labels = torch.cat(test_labels, dim=0).numpy()

acc = (knn.predict(test_feats) == test_labels).mean()
print(f"k-NN accuracy on STL-10 using DINO embeddings: {acc*100:.2f}%")

# Dimension of DINO features
with torch.no_grad():
    sample_x, _ = next(iter(train_loader))
    sample_x = sample_x.to(device)
    feat_dim = dino_features(model, sample_x).shape[1]
feat_dim

linear_head = nn.Linear(feat_dim, 10).to(device)  # STL-10 has 10 classes
optimizer = torch.optim.Adam(linear_head.parameters(), lr=1e-3)
criterion = nn.CrossEntropyLoss()

def train_linear_probe(epochs=10):
    model.eval()
    linear_head.train()

    for ep in range(epochs):
        total_loss = 0
        total_correct = 0
        total_count = 0
        for imgs, labels in train_loader:
            imgs, labels = imgs.to(device), labels.to(device)

            with torch.no_grad():
                feats = dino_features(model, imgs)  # [B,C]

            logits = linear_head(feats)
            loss = criterion(logits, labels)

            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

            total_loss += loss.item() * imgs.size(0)
            preds = logits.argmax(1)
            total_correct += (preds == labels).sum().item()
            total_count += imgs.size(0)

        print(f"Epoch {ep+1}/{epochs} | Loss {total_loss/total_count:.4f} | "
              f"Train Acc {total_correct/total_count*100:.2f}%")

def eval_linear_probe():
    model.eval()
    linear_head.eval()
    total_correct = 0
    total_count = 0
    with torch.no_grad():
        for imgs, labels in test_loader:
            imgs, labels = imgs.to(device), labels.to(device)
            feats = dino_features(model, imgs)
            logits = linear_head(feats)
            preds = logits.argmax(1)
            total_correct += (preds == labels).sum().item()
            total_count += imgs.size(0)
    print(f"Linear probe test accuracy: {total_correct/total_count*100:.2f}%")

train_linear_probe(epochs=10)
eval_linear_probe()

# store train set features for retrieval
train_feats = []
train_labels = []
train_imgs_raw = []   # for visualization

for img_tensor, label in stl_train:
    x = img_tensor.unsqueeze(0).to(device)
    with torch.no_grad():
        feat = dino_features(model, x)[0].cpu()
    train_feats.append(feat)
    train_labels.append(label)
    train_imgs_raw.append(img_tensor)  # still normalized

train_feats = torch.stack(train_feats, dim=0)  # [N,C]
train_labels = torch.tensor(train_labels)
print("Train feats:", train_feats.shape)

@torch.no_grad()
def retrieve_similar_images(query_img_tensor, k=5):
    x = query_img_tensor.unsqueeze(0).to(device)
    q_feat = dino_features(model, x)[0].cpu()        # [C]

    # cosine similarity against all train features
    feats_norm = F.normalize(train_feats, dim=1)
    q_norm = F.normalize(q_feat.unsqueeze(0), dim=1)
    sims = (feats_norm @ q_norm.T).squeeze(1)        # [N]

    topk_vals, topk_idx = torch.topk(sims, k=k)
    return topk_idx, topk_vals

def visualize_retrieval_example():
    # pick random test image as query
    q_tensor, q_label = stl_test[np.random.randint(len(stl_test))]
    idxs, sims = retrieve_similar_images(q_tensor, k=5)

    q_pil = tensor_to_pil(q_tensor)

    retrieved_imgs = [tensor_to_pil(train_imgs_raw[i]) for i in idxs]
    titles = [f"sim={sims[i].item():.2f}, label={train_labels[idxs[i]].item()}"
              for i in range(len(idxs))]

    print("Query label:", q_label)
    show_side_by_side([q_pil], ["Query"], figsize=(3,3))
    show_side_by_side(retrieved_imgs, titles, figsize=(15,3))

visualize_retrieval_example()

from collections import defaultdict

def make_few_shot_subset(dataset, shots_per_class=5):
    class_to_imgs = defaultdict(list)
    for img, label in dataset:
        class_to_imgs[label].append((img, label))

    few_imgs = []
    few_labels = []
    for lbl, items in class_to_imgs.items():
        for img, lab in items[:shots_per_class]:
            few_imgs.append(img)
            few_labels.append(lab)

    return few_imgs, few_labels

few_imgs, few_labels = make_few_shot_subset(stl_train, shots_per_class=5)
print("Few-shot subset size:", len(few_imgs))

# extract features
few_feats = []
for img, lab in zip(few_imgs, few_labels):
    x = img.unsqueeze(0).to(device)
    with torch.no_grad():
        feat = dino_features(model, x)[0].cpu()
    few_feats.append(feat)

few_feats = torch.stack(few_feats, dim=0).numpy()
few_labels = np.array(few_labels)

knn_few = KNeighborsClassifier(n_neighbors=1, metric="cosine")
knn_few.fit(few_feats, few_labels)

# evaluate on full test set
test_feats = []
test_labels = []

for imgs, labels in test_loader:
    imgs = imgs.to(device)
    with torch.no_grad():
        f = dino_features(model, imgs).cpu()
    test_feats.append(f)
    test_labels.append(labels)

test_feats = torch.cat(test_feats, dim=0).numpy()
test_labels = torch.cat(test_labels, dim=0).numpy()

acc_few = (knn_few.predict(test_feats) == test_labels).mean()
print(f"Few-shot 5-shot 1-NN accuracy: {acc_few*100:.2f}%")